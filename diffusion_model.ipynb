{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShreySharma07/DDPM-Denoising-Diffusion-Probabilistic-Model-/blob/main/diffusion_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWm7l-XTw2Bs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True"
      ],
      "metadata": {
        "id": "hzk_hK11w8UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 32 #for cifar\n",
        "image_channels = 3  # RGB images\n",
        "timesteps = 1000  # Increased for better quality\n",
        "batch_size = 256   # Reduced for better training stability\n",
        "num_epochs = 500\n",
        "lr = 2e-5\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "r7yvdONtw_nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    \"\"\"\n",
        "    Cosine schedule as proposed in Improved DDPMs.\n",
        "    \"\"\"\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0.0001, 0.9999)"
      ],
      "metadata": {
        "id": "S3juji0VxBr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "betas = cosine_beta_schedule(timesteps)\n",
        "alphas = 1.0 - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
        "posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)"
      ],
      "metadata": {
        "id": "fMtyXIVoxDtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_diffusion_sample(x_0, t, device):\n",
        "    \"\"\"Add noise to images\"\"\"\n",
        "    noise = torch.randn_like(x_0)\n",
        "    sqrt_alphas_cumprod_t = sqrt_alphas_cumprod.to(device)[t].view(-1, 1, 1, 1)\n",
        "    sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod.to(device)[t].view(-1, 1, 1, 1)\n",
        "\n",
        "    return sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise, noise\n"
      ],
      "metadata": {
        "id": "8AhUiZV-xF1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sinusoidal_embeddings(timesteps, embedding_dim):\n",
        "    half_dim = embedding_dim // 2\n",
        "    emb = math.log(10000) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, device=timesteps.device) * -emb)\n",
        "    emb = timesteps[:, None] * emb[None, :]\n",
        "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
        "    return emb\n"
      ],
      "metadata": {
        "id": "Pl367-m5xIq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dim, num_groups=8):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_emb_dim, out_channels)\n",
        "        )\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.GroupNorm(num_groups, in_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.GroupNorm(num_groups, out_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        )\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.residual_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
        "        else:\n",
        "            self.residual_conv = nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb):\n",
        "      h = self.block1(x)\n",
        "      time_emb = self.time_mlp(time_emb)\n",
        "      h += time_emb[:, :, None, None]\n",
        "      h = self.block2(h)\n",
        "      return h + self.residual_conv(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, channels, num_groups=8):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.norm = nn.GroupNorm(num_groups, channels)\n",
        "        self.q = nn.Conv2d(channels, channels, 1)\n",
        "        self.k = nn.Conv2d(channels, channels, 1)\n",
        "        self.v = nn.Conv2d(channels, channels, 1)\n",
        "        self.out = nn.Conv2d(channels, channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        h = self.norm(x)\n",
        "\n",
        "        q = self.q(h).view(B, C, H*W).transpose(1, 2)\n",
        "        k = self.k(h).view(B, C, H*W)\n",
        "        v = self.v(h).view(B, C, H*W).transpose(1, 2)\n",
        "\n",
        "        attn = torch.softmax(torch.bmm(q, k) / math.sqrt(C), dim=-1)\n",
        "        h = torch.bmm(attn, v).transpose(1, 2).view(B, C, H, W)\n",
        "\n",
        "        return x + self.out(h)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, time_emb_dim=128, base_channels=128, channel_mults=(1, 2, 4, 8)):\n",
        "      super().__init__()\n",
        "\n",
        "      self.time_emb_dim = time_emb_dim\n",
        "      self.base_channels = base_channels # Increased from 64\n",
        "      self.channel_mults = channel_mults\n",
        "\n",
        "      # Time embedding\n",
        "      self.time_mlp = nn.Sequential(\n",
        "          nn.Linear(time_emb_dim, time_emb_dim * 4),\n",
        "          nn.SiLU(),\n",
        "          nn.Linear(time_emb_dim * 4, time_emb_dim)\n",
        "      )\n",
        "\n",
        "      # Initial convolution\n",
        "      self.init_conv = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n",
        "\n",
        "      # Encoder\n",
        "      self.downs = nn.ModuleList()\n",
        "      in_chs = base_channels\n",
        "      for i, mult in enumerate(channel_mults):\n",
        "          out_chs = base_channels * mult\n",
        "          is_last = (i == len(channel_mults) - 1)\n",
        "          self.downs.append(nn.ModuleList([\n",
        "              ResidualBlock(in_chs, out_chs, time_emb_dim),\n",
        "              ResidualBlock(out_chs, out_chs, time_emb_dim),\n",
        "              AttentionBlock(out_chs) if mult in [4, 8] else nn.Identity(), # Apply attention at more levels\n",
        "              nn.Conv2d(out_chs, out_chs, 3, stride=2, padding=1) if not is_last else nn.Identity()\n",
        "          ]))\n",
        "          in_chs = out_chs\n",
        "\n",
        "      # Middle\n",
        "      mid_channels = base_channels * channel_mults[-1]\n",
        "      self.middle = nn.ModuleList([\n",
        "          ResidualBlock(mid_channels, mid_channels, time_emb_dim),\n",
        "          AttentionBlock(mid_channels),\n",
        "          ResidualBlock(mid_channels, mid_channels, time_emb_dim)\n",
        "      ])\n",
        "\n",
        "      # Decoder\n",
        "      self.ups = nn.ModuleList()\n",
        "      for i in reversed(range(len(channel_mults))):\n",
        "          mult = channel_mults[i]\n",
        "          out_chs = base_channels * mult\n",
        "\n",
        "          prev_level_chs = base_channels * channel_mults[min(i + 1, len(channel_mults) - 1)]\n",
        "          skip_conn_chs = base_channels * channel_mults[i]\n",
        "          skip_conn_chs = base_channels * channel_mults[i]\n",
        "          input_chs_resblock = prev_level_chs + skip_conn_chs\n",
        "\n",
        "          is_first_decoder_block = (i == len(channel_mults) - 1)\n",
        "          upsample_layer = nn.Identity() if is_first_decoder_block else nn.ConvTranspose2d(prev_level_chs, prev_level_chs, 2, stride=2)\n",
        "\n",
        "          self.ups.append(nn.ModuleList([\n",
        "              ResidualBlock(input_chs_resblock, out_chs, time_emb_dim),\n",
        "              ResidualBlock(out_chs, out_chs, time_emb_dim),\n",
        "              AttentionBlock(out_chs) if mult in [4, 8] else nn.Identity(), # Symmetrical attention\n",
        "              upsample_layer\n",
        "          ]))\n",
        "\n",
        "      # Output\n",
        "      self.out = nn.Sequential(\n",
        "          nn.GroupNorm(8, base_channels),\n",
        "          nn.SiLU(),\n",
        "          nn.Conv2d(base_channels, out_channels, 3, padding=1)\n",
        "      )\n",
        "\n",
        "    # highlight-start\n",
        "    def forward(self, x, t):\n",
        "      # Time embedding\n",
        "      if len(t.shape) == 0:\n",
        "          t = t.unsqueeze(0)\n",
        "      t_emb = get_sinusoidal_embeddings(t.float(), self.time_emb_dim).to(x.device)\n",
        "      t_emb = self.time_mlp(t_emb)\n",
        "\n",
        "      # Initial conv\n",
        "      x = self.init_conv(x)\n",
        "\n",
        "      # Encoder\n",
        "      skips = []\n",
        "      for resblock1, resblock2, attnblock, downsample in self.downs:\n",
        "          x = resblock1(x, t_emb)\n",
        "          x = resblock2(x, t_emb)\n",
        "          x = attnblock(x)\n",
        "          skips.append(x)\n",
        "          x = downsample(x)\n",
        "\n",
        "\n",
        "      # Middle\n",
        "      for block in self.middle:\n",
        "          if isinstance(block, ResidualBlock):\n",
        "               x = block(x, t_emb)\n",
        "          else:\n",
        "               x = block(x)\n",
        "\n",
        "      # Decoder\n",
        "      for i, (resblock1, resblock2, attnblock, upsample) in enumerate(self.ups):\n",
        "          skip = skips.pop()\n",
        "          x = upsample(x)\n",
        "          x = torch.cat([x, skip], dim=1) # Concatenate skip connection\n",
        "          x = resblock1(x, t_emb)\n",
        "          x = resblock2(x, t_emb)\n",
        "          x = attnblock(x)\n",
        "\n",
        "      # Output\n",
        "      return self.out(x)\n",
        "    # highlight-end"
      ],
      "metadata": {
        "id": "vH3tpw-WxJp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])"
      ],
      "metadata": {
        "id": "vWrBMIX1yFbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def sample_timestep(x, t, model):\n",
        "    \"\"\"Sample single timestep\"\"\"\n",
        "    if isinstance(t, int):\n",
        "        t = torch.tensor([t] * x.shape[0], device=x.device)\n",
        "\n",
        "    betas_t = betas.to(x.device)[t].view(-1, 1, 1, 1)\n",
        "    sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod.to(x.device)[t].view(-1, 1, 1, 1)\n",
        "    sqrt_recip_alphas_t = sqrt_recip_alphas.to(x.device)[t].view(-1, 1, 1, 1)\n",
        "\n",
        "    predicted_noise = model(x, t)\n",
        "\n",
        "    model_mean = sqrt_recip_alphas_t * (x - betas_t * predicted_noise / sqrt_one_minus_alphas_cumprod_t)\n",
        "\n",
        "    if t[0] == 0:\n",
        "        return model_mean\n",
        "    else:\n",
        "        posterior_variance_t = posterior_variance.to(x.device)[t].view(-1, 1, 1, 1)\n",
        "        noise = torch.randn_like(x)\n",
        "        return model_mean + torch.sqrt(posterior_variance_t) * noise"
      ],
      "metadata": {
        "id": "sCdCuZdeyHm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate_samples(model, num_samples, device):\n",
        "    model.eval()\n",
        "\n",
        "    # Start with pure noise\n",
        "    x = torch.randn(num_samples, image_channels, image_size, image_size, device=device)\n",
        "\n",
        "    # Denoise step by step\n",
        "    for i in tqdm(reversed(range(timesteps)), desc='Sampling'):\n",
        "        x = sample_timestep(x, i, model)\n",
        "\n",
        "    # Convert from [-1, 1] to [0, 1]\n",
        "    x = (x.clamp(-1, 1) + 1) / 2\n",
        "    return x"
      ],
      "metadata": {
        "id": "nENCKlVIyJxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    root='./data',        # Directory to store the data\n",
        "    train=True,           # Use the training set\n",
        "    download=True,        # Download if not found\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjYMMDCFyLih",
        "outputId": "0964cd73-9067-4b77-a662-204dc416021c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet(in_channels=image_channels, out_channels=image_channels)\n",
        "\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f'lets use {torch.cuda.device_count()} gpus')\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "model = torch.compile(model)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n"
      ],
      "metadata": {
        "id": "3zvaDPepyOeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import GradScaler, autocast\n",
        "# Before the loop\n",
        "scaler = GradScaler()\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "    for batch_idx, (images, _) in enumerate(pbar):\n",
        "        images = images.to(device)\n",
        "        t = torch.randint(0, timesteps, (images.shape[0],), device=device)\n",
        "\n",
        "        # highlight-start\n",
        "        with autocast():\n",
        "            # Forward pass runs in mixed precision\n",
        "            noisy_images, noise = forward_diffusion_sample(images, t, device)\n",
        "            predicted_noise = model(noisy_images, t)\n",
        "            loss = F.mse_loss(predicted_noise, noise)\n",
        "        # highlight-end\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        # highlight-start\n",
        "        # Scales loss. Calls backward() on scaled loss to create scaled gradients.\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "\n",
        "        # Clip the unscaled gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Unscales the gradients of optimizer's assigned params.\n",
        "        scaler.step(optimizer)\n",
        "        # Updates the scale for next iteration.\n",
        "        scaler.update()\n",
        "        # highlight-end\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "    scheduler.step()\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "\n",
        "    # Generate samples every 25 epochs\n",
        "    if (epoch + 1) % 25 == 0:\n",
        "        print(f\"\\nEpoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
        "        samples = generate_samples(model, 8, device)\n",
        "\n",
        "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "        for i, ax in enumerate(axes.flat):\n",
        "            ax.imshow(samples[i].cpu().permute(1, 2, 0))\n",
        "            ax.axis('off')\n",
        "\n",
        "        plt.suptitle(f'Generated Samples - Epoch {epoch+1}')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Save checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_loss,\n",
        "        }, f'landscape_diffusion_epoch_{epoch+1}.pth')\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiOxAfDnyQX3",
        "outputId": "02e31737-442e-479a-dd15-9dac4bd85ad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-28-933878199.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "Epoch 1/500:   0%|          | 0/196 [00:00<?, ?it/s]/tmp/ipython-input-28-933878199.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "W0723 10:19:49.215000 584 torch/_inductor/utils.py:1137] [1/0] Not enough SMs to use max_autotune_gemm mode\n",
            "Epoch 1/500:   1%|          | 1/196 [01:56<6:19:16, 116.70s/it, loss=1.0630]/tmp/ipython-input-28-933878199.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Epoch 1/500:  99%|█████████▉| 195/196 [04:29<00:00,  1.26it/s, loss=0.1238]"
          ]
        }
      ]
    }
  ]
}